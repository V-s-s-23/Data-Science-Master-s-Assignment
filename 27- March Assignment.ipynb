{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "445073e8",
   "metadata": {},
   "source": [
    "# Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "## Answer:- R-squared is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variable in a linear regression model. It is calculated as the ratio of the explained variance to the total variance and ranges from 0 to 1.\n",
    "## R2= 1- SSres / SStot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61f1ab",
   "metadata": {},
   "source": [
    "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "## Answer:- Adjusted R-squared is a modified version of R-squared that adjusts for the number of independent variables in a linear regression model. Unlike R-squared, it penalizes the inclusion of irrelevant variables and always decreases when a new variable is added, providing a more accurate measure of the model's goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb484fb",
   "metadata": {},
   "source": [
    "# Q3. When is it more appropriate to use adjusted R-squared?\n",
    "## Answer:- Generally we use adjusted R-squared when our datset contains many features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3bcca4",
   "metadata": {},
   "source": [
    "# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "## Answer:- In the context of regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are metrics used to evaluate the performance of a regression model. RMSE and MSE measure the average squared difference between the predicted and actual values, with RMSE taking the square root of the MSE to return the error in the same units as the target variable. MAE measures the average absolute difference between the predicted and actual values. All three metrics are commonly used to compare the performance of different regression models, with lower values indicating better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab02ea41",
   "metadata": {},
   "source": [
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "## Ansewr:- Advantages of using RMSE and MSE include their sensitivity to outliers and ability to penalize large errors, making them useful in applications where accurate prediction is critical. However, they are more complex to interpret and may be affected by differences in the scale of the dependent variable.\n",
    "## On the other hand, MAE is easier to interpret and less affected by outliers, making it useful in applications where prediction accuracy is less critical. However, it may not capture the full range of errors and tends to be less sensitive to larger errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb1fff2",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "## Answer:- Lasso regularization is a method used in linear regression to prevent overfitting by adding a penalty term to the loss function. This penalty term encourages the model to have a sparse solution, i.e., to have many of its coefficients equal to zero. In contrast to Ridge regularization, Lasso selects a subset of the features most relevant to the model, effectively performing feature selection. Lasso is particularly useful when dealing with high-dimensional data, where many features may be irrelevant. If the goal is to identify the most important features and obtain a sparse model, Lasso is the better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe32c5",
   "metadata": {},
   "source": [
    "# Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "## Answer:- Regularized linear models add a penalty term to the loss function to reduce the magnitude of the model coefficients, which helps to prevent overfitting. For example, Lasso regression adds an L1 penalty, which results in sparse solutions by setting some coefficients to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4d3f7",
   "metadata": {},
   "source": [
    "# 8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "## Answer:- Regularized linear models have limitations in handling non-linear relationships and can be sensitive to outliers. Additionally, the choice of regularization parameter is subjective and can affect model performance. As such, they may not always be the best choice for regression analysis and should be evaluated against alternative models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c362c973",
   "metadata": {},
   "source": [
    "# Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "## Answer:- I would choose Model B as the better performer because it has a lower MAE. However, the choice of metric depends on the context and goals of the analysis. Both RMSE and MAE have their limitations, and it is important to consider them when selecting an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887ba00",
   "metadata": {},
   "source": [
    "# Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "## Answer:- The better performer would depend on the specific dataset and goals of the model. However, in general, Lasso regularization with a higher regularization parameter tends to result in more sparse models with some coefficients being exactly zero, which can aid in feature selection. Ridge regularization with a lower regularization parameter tends to result in smoother coefficients and better stability. Trade-offs include the interpretability of the coefficients, potential for overfitting or underfitting, and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653cf5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
