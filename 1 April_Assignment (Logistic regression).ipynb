{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efb23cb",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate.\n",
    "## Answer:- The major difference between \"Linear regression\" and \"Logistic regression\" is linear regression is used to solve regression based problem and Logistic regression is used for classification problems.\n",
    "## Example:- 1. linear regression - Predicting Price of house\n",
    "##           2. Logistic regression - Predicting Weather student will pass or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43935ad6",
   "metadata": {},
   "source": [
    "# Q2. What is the cost function used in logistic regression, and how is it optimized?\n",
    "## Anwer:- The cost function used in logistic regression is the log loss function, also known as the cross-entropy loss function. It measures the difference between the predicted probabilities and the actual binary labels, penalizing the model for incorrect predictions. The formula for the log loss function is:\n",
    "## L(y, 天) = -(y log(天) + (1-y) log(1-天))\n",
    "## where y is the true label (0 or 1), 天 is the predicted probability, and log is the natural logarithm.\n",
    "## To optimize the cost function, gradient descent is commonly used. The goal is to find the set of parameters that minimize the log loss function by iteratively adjusting the weights in the model. The gradient of the cost function with respect to the parameters is calculated, and the parameters are updated in the direction of the negative gradient, until convergence is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1504a",
   "metadata": {},
   "source": [
    "#  Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n",
    "## Answer:- Regularization is a technique used in logistic regression to prevent overfitting, which occurs when the model fits the training data too closely and performs poorly on new data. Regularization adds a penalty term to the loss function that is optimized during model training. This penalty discourages the model from assigning high weights to certain features, effectively reducing the complexity of the model. By reducing complexity, regularization can prevent the model from memorizing the training data and improve its ability to generalize to new data. The two most common forms of regularization are L1 and L2 regularization, which add either the absolute values or the squared values of the weights to the loss function, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cccfce1",
   "metadata": {},
   "source": [
    "# Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?\n",
    "## Answer:- The ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classifier, such as logistic regression, at different classification thresholds. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR), where the TPR is the ratio of correctly classified positive instances to all positive instances, and the FPR is the ratio of incorrectly classified negative instances to all negative instances. The area under the ROC curve (AUC) is a commonly used metric to evaluate the overall performance of the logistic regression model, where a higher AUC indicates better performance. The ROC curve is also useful for determining the optimal threshold for classification, balancing the trade-off between the TPR and FPR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7a574a",
   "metadata": {},
   "source": [
    "# Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?\n",
    "## Answer:- Some common techniques for feature selection in logistic regression include forward selection, backward elimination, and Lasso regularization.Forward selection involves adding features to the model one by one until no additional improvements in the model's performance are observed. Backward elimination, on the other hand, involves removing features from the model until no further improvements are observed. Lasso regularization uses a penalty term that shrinks the coefficients of less important features towards zero, effectively eliminating them from the model. These techniques help improve the model's performance by reducing the complexity of the model, avoiding overfitting, and improving the interpretability of the model by focusing on the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0eaff3",
   "metadata": {},
   "source": [
    "# Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?\n",
    "## Answer:- 1.To overcome from this problem we can use undersampling & oversampling.\n",
    "##                 2.We can also use ensamble techniques like bagging and boosting.\n",
    "##                 3.We can Stratified cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61f110",
   "metadata": {},
   "source": [
    "# Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?\n",
    "## Answer:- One common issue with logistic regression is multicollinearity among independent variables, which can inflate standard errors and lead to unreliable estimates. This can be addressed by using techniques such as ridge regression or principal component analysis to reduce the collinearity. Another issue is overfitting, which can be addressed by using regularization methods such as L1 or L2 regularization. Other challenges include handling missing data, selecting relevant features, and dealing with class imbalance, which can be addressed using appropriate techniques such as imputation, feature selection, and sampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e429ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
